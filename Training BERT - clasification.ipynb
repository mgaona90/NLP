{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ed5ffec0-3fac-46e7-9f90-426eb8ea0c3c","showTitle":false,"title":""}},"outputs":[],"source":["%pip install transformers==4.4\n","%pip install torch==1.9.0\n","%pip install spacy==3.1.3"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cec61d6d-ea1a-4d2f-9ee6-625393a24aa5","showTitle":false,"title":""}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import torch\n","import numpy as np\n","import pandas as pd\n","import re\n","import datetime as dt\n","\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup,BertForMaskedLM, BertTokenizer\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.model_selection import train_test_split\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from textwrap import wrap\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"bd36ec92-d1c4-416c-bf61-66f95f8eec7b","showTitle":false,"title":""}},"outputs":[],"source":["#################  COPIAR ARCHIVOS EN SYNAPSE DESDE DATALAKE\n","#Correr el comando de abajo solo cuando se quiera modificar algun archivo como por ejemplo el dataset. Modificando los nombres de los archivos para copiar de datalake a synapse y poder usarlo aca.\n","\n","# dbutils.fs.cp('/tabla param_temas_tedsteo.csv','/tablas param_temas_testeo.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"32cc3f4b-9573-4536-af62-8ded9b8540f4","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","DROP TABLE IF EXISTS PRUEBA_DATASET;\n","CREATE TABLE PRUEBA_DATASET\n","USING CSV\n","LOCATION '/Temas/Dataset_temas_11.5s.2022.csv';"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5eaba5dc-2a0f-4276-b936-2508d792bcac","showTitle":false,"title":""}},"outputs":[],"source":["%sql\n","select * from PRUEBA_DATASET\n","limit 10"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"935b59ae-3c5e-457d-8173-edb952994626","showTitle":false,"title":""}},"outputs":[],"source":["# Definición de Parametros\n","\n","dbutils.widgets.text(\"max_lenght\", \"\",\"\")\n","dbutils.widgets.text(\"batch_size\", \"\",\"\")\n","dbutils.widgets.text(\"epochs\", \"\",\"\")\n","dbutils.widgets.text(\"test_size\", \"\",\"\")\n","dbutils.widgets.text(\"Nombre_archivo_dataset\", \"\",\"\")\n","dbutils.widgets.text(\"Nombre_archivo_testeo\", \"\",\"\")\n","\n","MAX_LEN = int(dbutils.widgets.get(\"max_lenght\"))\n","BATCH_SIZE = int(dbutils.widgets.get(\"batch_size\"))\n","EPOCHS = int(dbutils.widgets.get(\"epochs\"))\n","TEST_SIZE = float(dbutils.widgets.get(\"test_size\"))\n","Nombre_archivo_dataset = dbutils.widgets.get(\"Nombre_archivo_dataset\")\n","Nombre_archivo_testeo=dbutils.widgets.get(\"Nombre_archivo_testeo\") #testeo_temas.11.05.2022"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"51fd8a9f-bef3-4fbd-90ce-8531f5f71205","showTitle":false,"title":""}},"outputs":[],"source":["# Carga de data y configuraciones iniciales\n","RANDOM_SEED = 42\n","df = pd.read_csv(f\"/Datasets/{Nombre_archivo_dataset}.csv\", sep=';',encoding = 'unicode_escape')\n","df.columns=['text','label']\n","df=df[~df['label'].isna()]\n","le = LabelEncoder()\n","df['category'] = le.fit_transform(df['label']) #revisar con los indices existenes si coinciden.\n","dict_label_category=dict(zip(df['label'],df['category']))\n","dict_label_category_inverse=dict(zip(df['category'],df['label']))\n","NCLASSES = len(df['category'].unique())\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","df['comentario_inicial']=df['text']\n","df=df[df['label']!=99]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6c200352-a403-494f-959c-f86e84800e0a","showTitle":false,"title":""}},"outputs":[],"source":["  # Tratamiento de datos\n","def tratamiento_texto (dataframe,col):\n","    dataframe[col]=dataframe[col].apply(lambda x:str(x).lower())\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('¡','i',x)) #encoding = 'unicode_escape' no identifica al caracter í\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub(' dev/.',' devolución',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub(' inf/.',' información',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub(' doc/.',' documentos',x)) \n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('ivr','encuesta telefónica',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('1m21|2m21','encuesta',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:    re.sub('/|corresponsal bancario -|1-|1|cliente corporativo|cliente natural|cliente|corporativo|usuario final|colaborado de red|p_|q_|in_|s_',' ',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('tu cuenta|payty','addidas',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('pap ','punto de atención ',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('efecty','empresa',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('gyg|g&f','giros y finanzas',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('Web App','aplicación web',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('movii','empresa movil',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('pqr|pqrs','petición o queja',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('^me ','modificaciones especiales',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('^mb ','modificaciones básicas',x))\n","    dataframe[col]=dataframe[col].apply(lambda x: re.sub('(\\|)|\\(|\\)|_|-',' ',x))\n","    dataframe[col]=dataframe[col].apply(lambda x:   re.sub('   |  ',' ',x))\n","\n","# ME, consultando a clau\n","tratamiento_texto(df,'text')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"21ae3008-e8aa-4b62-8a8c-6333bab14f5f","showTitle":false,"title":""}},"outputs":[],"source":["# Traigo el tokenizer de la libreria transformers\n","tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n","sample_txt = 'prueba'\n","sample_txt=sample_txt.lower()\n","tokens = tokenizer.tokenize(sample_txt)\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","############################################# CODIFICACION encoder\n","encoding = tokenizer.encode_plus(\n","    sample_txt,\n","    max_length = MAX_LEN, \n","    truncation = True,\n","    add_special_tokens = True,\n","    return_token_type_ids = False,\n","    # padding=True,\n","    pad_to_max_length = True,\n","    return_attention_mask = True,\n","    return_tensors = 'pt')\n","\n","############################################# CREACIÓN DATASET\n","class IMDBDataset(Dataset):\n","\n","  def __init__(self,reviews,categories,tokenizer,max_len):\n","    self.reviews = reviews\n","    self.categories = categories\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __len__(self):\n","      return len(self.reviews)\n","    \n","  def __getitem__(self, item):\n","    review = str(self.reviews[item])\n","    category = self.categories[item]\n","    encoding = tokenizer.encode_plus(\n","        review,\n","        max_length = self.max_len,\n","        truncation = True,\n","        add_special_tokens = True,\n","        return_token_type_ids = False,\n","        # padding=True,\n","        pad_to_max_length = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt'\n","        )\n","    \n","    return {\n","          'review': review,\n","          'input_ids': encoding['input_ids'].flatten(),\n","          'attention_mask': encoding['attention_mask'].flatten(),\n","          'category': torch.tensor(category, dtype=torch.long)\n","      } \n","    \n","############################################# DATA LOADER. data que entra al modelo\n","\n","def data_loader(df, tokenizer, max_len, batch_size):\n","  dataset = IMDBDataset(\n","      reviews = df.text.to_numpy(),\n","      categories = df['category'].to_numpy(),\n","      tokenizer = tokenizer,\n","      max_len = MAX_LEN\n","  )\n","\n","  return DataLoader(dataset, batch_size = BATCH_SIZE, num_workers = 4)\n","\n","\n","############################################# EL MODELO\n","class BERTSentimentClassifier(nn.Module):\n","\n","  def __init__(self, n_classes):\n","    super(BERTSentimentClassifier, self).__init__()\n","    # self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","    # self.bert = BertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n","    self.bert = BertModel.from_pretrained(\"/dbfs/Efecty_VoC/BETOmodel/BETO/\")\n","    self.drop = nn.Dropout(p=0.2)\n","    self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n","\n","  def forward(self, input_ids, attention_mask):\n","    _, cls_output = self.bert(\n","        input_ids = input_ids,\n","        attention_mask = attention_mask,\n","        return_dict=False\n","    )\n","    drop_output = self.drop(cls_output)\n","    output = self.linear(drop_output)\n","    return output\n","  \n","model = BERTSentimentClassifier(NCLASSES)\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8dc6b1ff-8cf4-4071-b401-694fecd31a9a","showTitle":false,"title":""}},"outputs":[],"source":["df_train, df_test = train_test_split(df, test_size = TEST_SIZE, random_state=RANDOM_SEED)\n","\n","train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8b10f773-1301-489e-84d2-726b6e355836","showTitle":false,"title":""}},"outputs":[],"source":["############################################# DEFINICION DEL ENTRENAMIENTO\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps = 0,\n","    num_training_steps = total_steps\n",")\n","loss_fn = nn.CrossEntropyLoss().to(device)\n","\n","#############################################  ITERACION ENTRENAMIENTO\n","def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n","  model = model.train()\n","  losses = []\n","  correct_predictions = 0\n","  for batch in data_loader:\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    categories = batch['category'].to(device)\n","    outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n","    _, preds = torch.max(outputs, dim=1)\n","    loss = loss_fn(outputs, categories)\n","    correct_predictions += torch.sum(preds == categories)\n","    losses.append(loss.item())\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","  return correct_predictions.double()/n_examples, np.mean(losses)\n","\n","############################################# ITERACION EVALUACION\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","  model = model.eval()\n","  losses = []\n","  correct_predictions = 0\n","  with torch.no_grad():\n","    for batch in data_loader:\n","      input_ids = batch['input_ids'].to(device)\n","      attention_mask = batch['attention_mask'].to(device)\n","      categories = batch['category'].to(device)\n","      outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n","      _, preds = torch.max(outputs, dim=1)\n","      loss = loss_fn(outputs, categories)\n","      correct_predictions += torch.sum(preds == categories)\n","      losses.append(loss.item())\n","  return correct_predictions.double()/n_examples, np.mean(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"be255757-9441-49ef-ac5a-0b19d8f2f26b","showTitle":false,"title":""}},"outputs":[],"source":["############################################# ENTRENAMIENTO X EPOCH\n","\n","for epoch in range(EPOCHS):\n","  print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n","  print('------------------')\n","  train_acc, train_loss = train_model(\n","      model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train)\n","  )\n","  test_acc, test_loss = eval_model(\n","      model, test_data_loader, loss_fn, device, len(df_test)\n","  )\n","  print('Entrenamiento: Loss: {}, accuracy: {}'.format(train_loss, train_acc))\n","  print('Validación: Loss: {}, accuracy: {}'.format(test_loss, test_acc))\n","  print('')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8e3bdf39-ce5e-4e59-ad3f-c1b73507b743","showTitle":false,"title":""}},"outputs":[],"source":["############################################# PREDICCION Y EVALUACIÓN\n","def predict(review_text):\n","  encoding_review = tokenizer.encode_plus(\n","      review_text,\n","      max_length = MAX_LEN,\n","      truncation = True,\n","      add_special_tokens = True,\n","      return_token_type_ids = False,\n","      # padding=True,\n","      pad_to_max_length = True,\n","      return_attention_mask = True,\n","      return_tensors = 'pt'\n","      )\n","  \n","  input_ids = encoding_review['input_ids'].to(device)\n","  attention_mask = encoding_review['attention_mask'].to(device)\n","  output = model(input_ids, attention_mask)\n","  _, prediction = torch.max(output, dim=1) \n","  return prediction.tolist()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"78218d06-a32d-4830-bacd-62d2bf4aa120","showTitle":false,"title":""}},"outputs":[],"source":["############################################# CONFUSION MATRIX\n","df['predic_categ']=df['text'].apply(lambda x: predict(x))\n","cm = confusion_matrix(df['category'],df['predic_categ'])\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot() \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"10bf6a9e-b5c3-4708-b440-04c18cd5781f","showTitle":false,"title":""}},"outputs":[],"source":["print(dict_label_category)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"609e4d09-0a92-464a-bda5-8f3064d5b2e4","showTitle":false,"title":""}},"outputs":[],"source":["\n","df_test = pd.read_csv(f\"/Datasets/{Nombre_archivo_testeo}.csv\", sep=';',encoding = 'unicode_escape')\n","df_test.columns=['origen','text','label']\n","df_test['text2']=df_test['text']\n","df_test=df_test[~df_test['label'].isna()]\n","tratamiento_texto(df_test,'text')\n","df_test['predic_categ_test']=df_test['text'].apply(lambda x: predict(x))\n","df_test['category'] = df_test['label'].apply(lambda x: dict_label_category.get(x))\n","df_test['predic_lab_test']=df_test['predic_categ_test'].apply(lambda x: dict_label_category_inverse.get(x))\n","\n","############################################# CONFUSION MATRIX\n","df_test.category.fillna(-99,inplace=True)\n","cm = confusion_matrix(df_test['category'],df_test['predic_categ_test'])\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot() \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e5248c0d-0aee-4e58-a77d-83d2cea24326","showTitle":false,"title":""}},"outputs":[],"source":["# df_test[50:100]\n","# df_test[df_test['category']==3]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fa7ac5a0-bd11-4906-af8e-e65a8e41dbd9","showTitle":false,"title":""}},"outputs":[],"source":["# df_test[df_test['category']==3]\n","\n","# df_test.to_csv(\"/dbfs/Efecty_VoC/BETOmodel/Datasets/para_tabla_parametrica.csv\")\n","# dbutils.fs.cp('/Efecty_VoC/BETOmodel/Datasets/para_tabla_parametrica.csv','/mnt/contenedor/Efecty_VoC/NLP Models/Datasets/para_tabla_parametrica.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"05e81203-edb8-4dd4-a26a-6964323ceabd","showTitle":false,"title":""}},"outputs":[],"source":["# Save model\n","name_file=f'/Modelo_Tema1_{str(dt.date.today())}.pkl'\n","# torch.save(model,name_file)\n","dbutils.fs.cp(f'/Modelo_Tema1_{str(dt.date.today())}.pkl',f'/mnt/contenedor/Efecty_VoC/NLP Models/Modelos/Modelo_Tema1_{str(dt.date.today())}.pkl')"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"tema1_Entrenamiento_modelo_NLP","notebookOrigID":2062501769275342,"widgets":{"Nombre_archivo_dataset":{"currentValue":"Dataset_temas_11.5.2022","nuid":"c50fe07d-2c16-44eb-bf45-2ef49ff36551","widgetInfo":{"defaultValue":"","label":"","name":"Nombre_archivo_dataset","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"Nombre_archivo_testeo":{"currentValue":"tabla param_temas_testeo","nuid":"085a33e2-87c1-4a4d-84e3-af835ef5d159","widgetInfo":{"defaultValue":"","label":"","name":"Nombre_archivo_testeo","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"batch_size":{"currentValue":"10","nuid":"0024ed43-d940-4971-b17b-b790a5b64832","widgetInfo":{"defaultValue":"","label":"","name":"batch_size","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"epochs":{"currentValue":"5","nuid":"598d7a21-6954-41bb-a58b-3a73e167e28c","widgetInfo":{"defaultValue":"","label":"","name":"epochs","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"max_lenght":{"currentValue":"12","nuid":"354025af-45cd-43f3-9c8a-8281c9fbabb5","widgetInfo":{"defaultValue":"","label":"","name":"max_lenght","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"test_size":{"currentValue":"0.1","nuid":"4e95083b-3293-4920-98a7-17c503736c4f","widgetInfo":{"defaultValue":"","label":"","name":"test_size","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}}}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
