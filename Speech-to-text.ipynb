{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2061b49-eed1-4f57-9859-7debf1699b2c",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "### Load audio\n",
    "Puede ser mp3 o wav, puede ser mono o estereo. \n",
    "### Convert into normalize Array \n",
    "Vector de que representa para un periodo de tiempo la intensidad para cada frecuencia (frecuencia determina la dimensión del vector, si es más de un canal, tendrá 2 vectores cada periodo de tiempo). El periodo puede ser 1 seg o 20 Milisegundos (normalmente son de 20 milisegundos, como lo es este ejemplo). Si\n",
    "un vector tiene 16.000 Hz como frecuencia máxima, entonces tendra dim=16.000\n",
    "### Convert to spectogram (creo que en este ejemplo no se hace)\n",
    "Lo convierte en una imagen, el cual es lo que consume el modelo. Creo que se usa fast fourier transform. Despues torch lo transforma en un tensor.\n",
    "### Speech Model\n",
    "Ya con el input, aca estará el modelo que se entrenará para predecir fonemas para cada periodo de tiempo. Wav2Vec2 (ENCODER) es uno. Lo bueno de transformers, es que predice considerando el todo, no de forma independiente a los demás ponemas.\n",
    "Aca se podría hacer un finetuning con esto: https://huggingface.co/blog/fine-tune-xlsr-wav2vec2\n",
    "\n",
    "El output será 1 vector por cada 20 milisegundos, con la probabilidad de cada fonema (la dimensión va a depender de la cantidad de fonemas/vocabulario con el que contemos. Los labels). A eso se le puede hacer un argumax para extraer el mas probable, o correrle algo como beam search para que traiga las combinaciones mas probables considerando un modelo linguistico.\n",
    "\n",
    "### CTC language model (Beam Search)\n",
    "Aca es donde tengo que ivnestigar más. En wev2vec puede que este considerado ya. Lo que si hay que investigar, es beam search,\n",
    "que lo que hace es identificar las palabras más comunes en su totalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5f58d-adac-4434-b1e0-735c80d9ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar aplicaciónes de nose q cosa linux: https://apps.microsoft.com/store/detail/windows-subsystem-for-linux-preview/9P9TQF7MRM4R?hl=es-ad&gl=ad&icid=CatNavWindowsApps\n",
    "\n",
    "# info de decoder:\n",
    "# https://huggingface.co/blog/wav2vec2-with-ngram\n",
    "# https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Boosting_Wav2Vec2_with_n_grams_in_Transformers.ipynb#scrollTo=aJREUIdqs5ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229df9b-faae-40fc-8784-940e0bc118d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls interesantes\n",
    "\n",
    "Aca puede explicar como aplica CTC a wav2vec2: https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Boosting_Wav2Vec2_with_n_grams_in_Transformers.ipynb#scrollTo=dAerOhydrNFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09894891-9af7-4472-bb26-a235a8e8e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generar dataset con audios propios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183595d-fa0e-4030-961c-61db9783510e",
   "metadata": {},
   "source": [
    "Carga audio y lo convierte a array y nose si a spectogram tambien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4bc7d53-b8bc-49cf-a753-acb58ce33ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install audio2numpy\n",
    "import audio2numpy as a2n\n",
    "\n",
    "\n",
    "#Read the audio data from the file and load it into a 2D Numpy array. (can be .wav o .mp3)\n",
    "x,sr=a2n.audio_from_file(\"wqwq.wav\")\n",
    "# En este ejemplo (\"wqwq.wav\"), x, tiene una dimensión de 751.616, y el audio es de 46seg. Por lo que se cumple que, 751.616/16.000=46. cada segundo tiene un vector de 16.000. En cada segundo, hay una foto de 16.000 frecuencias con el valor de la intesnidad.\n",
    "\n",
    "#This array consists of a sequence of numbers, each representing a measurement of the intensity or amplitude of the sound at a particular moment in time. The number of such measurements is determined by the sampling rate. For instance, if the sampling rate was 44.1kHz, the Numpy array will have a single row of 44,100 numbers for 1 second of audio.\n",
    "# Audio can have one or two channels, known as mono or stereo, in common parlance. With two-channel audio, we would have another similar sequence of amplitude numbers for the second channel. In other words, our Numpy array will be 3D, with a depth of 2.\n",
    "\n",
    "Dataset=[[dict({'array':common_voice[0],'path':f'audio{1}','sampling_rate':common_voice[1]})],[dict({'array':common_voice[0],'path':f'audio{1}','sampling_rate':common_voice[1]})]]\n",
    "label=['hola lo estamos llamando','hola lo  estamos para aca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850bb0c3-bc84-41f3-8e61-e80b6e3c3e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20940a8b-189f-4c3e-ac47-740534ff8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to uniform dimensions: sample rate, channels, and duration\n",
    "\n",
    "We might have a lot of variation in our audio data items. Clips might be sampled at different rates, or have a different number of channels. The clips will most likely have different durations. As explained above this means that the dimensions of each audio item will be different.\n",
    "Since our deep learning models expect all our input items to have a similar size, we now perform some data cleaning steps to standardize the dimensions of our audio data. We resample the audio so that every item has the same sampling rate. We convert all items to the same number of channels. All items also have to be converted to the same audio duration. This involves padding the shorter sequences or truncating the longer sequences.\n",
    "If the quality of the audio was poor, we might enhance it by applying a noise-removal algorithm to eliminate background noise so that we can focus on the spoken audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13c46c-fdd5-445d-bae1-edcaba10c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation of raw audio\n",
    "\n",
    "# We could apply some data augmentation techniques to add more variety to our input data and help the model learn to generalize to a wider range of inputs. We could Time Shift our audio left or right randomly by a small percentage, or change the Pitch or the Speed of the audio by a small amount.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mel Spectrograms\n",
    "\n",
    "# This raw audio is now converted to Mel Spectrograms. A Spectrogram captures the nature of the audio as an image by decomposing it into the set of frequencies that are included in it.\n",
    "\n",
    "\n",
    "# MFCC\n",
    "\n",
    "# For human speech, in particular, it sometimes helps to take one additional step and convert the Mel Spectrogram into MFCC (Mel Frequency Cepstral Coefficients). MFCCs produce a compressed representation of the Mel Spectrogram by extracting only the most essential frequency coefficients, which correspond to the frequency ranges at which humans speak.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# So our model takes the Spectrogram images and outputs character probabilities for each timestep or ‘frame’ in that Spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f1ecd-3afe-4964-9c9a-eaaa81228afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290b5c1-dfed-4c0c-a2cd-748588921ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a31175-25c3-482b-9b32-babe772c073f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73cb6f-0f7c-48cf-b353-f96e279f598b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96768d9b-1f3c-4bbc-a9f6-14a2b807db21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72ad8acb-fecc-4e0c-bdd1-b86b2940a1c7",
   "metadata": {},
   "source": [
    "PROCESADOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49cfd21e-396d-4be8-a66c-20986d83891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESSOR.Se pueden crear o descargar de un preentrenado. Es quien guarda el vocabulario, dimensiones de entrada y salida, padding, mask, y ese tipo de cosas\n",
    "\n",
    "# 3 formas  (feature extractor and tokenizer are wrapped into a single Wav2Vec2Processor):\n",
    "#        -processors como  Wav2Vec2Processor (tokenizer + feature extractor) o Wav2Vec2ProcessorWithLM (tokenizer + feature extractor+ decoder)\n",
    "#        -tokenizer + feature extractor\n",
    "#        - También se le puede definir un decoder ()\n",
    "\n",
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from transformers import Wav2Vec2CTCTokenizer,Wav2Vec2FeatureExtractor\n",
    "\n",
    "#Opción 1, la mas simple\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\")\n",
    "# Wav2Vec2Processor:Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor and a Wav2Vec2 CTC tokenizer into a single processor.\n",
    "# should be used for padding and conversion into a tensor of type torch.\n",
    "\n",
    "\n",
    "\n",
    "#Opción 2, toekizer e featureExtractor\n",
    "\n",
    "#Define el label y su configuración\n",
    "tokenizer=Wav2Vec2CTCTokenizer.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\")\n",
    "\n",
    "#Define el output e input y las configruaciones\n",
    "FeatureExtracor=Wav2Vec2FeatureExtractor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\")\n",
    "\n",
    "tokenizer.get_vocab() #Para ver el vocabulario o modificarlo (son los posibles labels)\n",
    "\" \".join(sorted(processor.tokenizer.get_vocab()))  #Para ver el vocabulario o modificarlo (son los posibles labels)\n",
    "len(tokenizer.get_vocab()) #ver dim salida, o qty de labels posibles\n",
    "\n",
    "\n",
    "#Opción 3\n",
    "\n",
    "# pip install transformers==4.18.0\n",
    "# pip install pyctcdecode\n",
    "# pip install https://github.com/kpu/kenlm/archive/master.zip \n",
    "\n",
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\")\n",
    "# feature_extractor (Wav2Vec2FeatureExtractor) — An instance of Wav2Vec2FeatureExtractor. The feature extractor is a required input.\n",
    "# tokenizer (Wav2Vec2CTCTokenizer) — An instance of Wav2Vec2CTCTokenizer. The tokenizer is a required input.\n",
    "# decoder (pyctcdecode.BeamSearchDecoderCTC) — An instance of pyctcdecode.BeamSearchDecoderCTC. The decoder is a required input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325693ef-9a21-4119-a8a2-8f406417b79c",
   "metadata": {},
   "source": [
    "MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa74fbfb-232b-4b41-8770-dfa91f15561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS (https://huggingface.co/docs/transformers/model_doc/wav2vec2)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\")\n",
    "# Wav2Vec2ForCTC: se carga el modelo\n",
    "# AutoModelForCTC\n",
    "# Wav2Vec2Model\n",
    "# Wav2Vec2ForCTC\n",
    "# Wav2Vec2ForSequenceClassification\n",
    "# Wav2Vec2ForAudioFrameClassification\n",
    "# Wav2Vec2ForXVector\n",
    "# Wav2Vec2ForPreTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75406fcb-a070-4af8-adea-4ff3ebee6210",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1ea00e-1a42-4728-9efa-4debda437e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convierte el input en un tensor+attention mask. prepara para enviarlo al modelo\n",
    "inputs = processor(x, sampling_rate=16_000, return_tensors=\"pt\")\n",
    "# Ajusta formato (dejan de ser float, y son 2 tensor: inputs y mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b72502e7-0294-424a-ae4d-5a95bda3a299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  logits = model(**inputs).logits\n",
    "\n",
    "#El modelo procesa el input y genera para cada 20 milisegundos un vector con 41 valores. \n",
    "# En este caso hay un total de 2349 períodos, donde cada uno es un tensor de dimensión 41 (el vocab. labels posibles) con probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b3b1163-12ee-4d40-8ed6-64955fa0c276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aquí me aparece once cerotres cincuenta y cuatrolistsi gas natural propano aquí me aparece es consulta de base de datos once diecinueve noventa y cuatro y se hace con la referenciacon referencia si señol pero entonces es que así está cargado a nivel nacional ningún punto de servicio lo carga e con el código de barras'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hago el argmax. Para cada uno de los 2.349 periodos de tiempo en los 46 seg, se selecciona el fonema con mayor probabilidad\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "# Con logits, es donde puedo aplicar beam search\n",
    "\n",
    "\n",
    "#Según los fonemas maximos para cada periodo de tiempo, con batch_decode identifico los labels y aplico el diccionario\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "transcription[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51064da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53335fed-7064-47bf-a0d0-8ded523ea627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a7389-fd81-4d56-9d23-6c28b43ddab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee6e4d-d6a6-45a2-9c60-d606efcb5ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925aa5ee-68cb-4bf8-9b71-9f432d74e5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596be62a-fbb9-4a73-92a3-c1f40113e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como saber como subdividir/segmentar el audio para que prediga los fonemas? cada fonema puede durar distinto, hay pausas, algunos van juntos, apple como sabe que son 2 p?\n",
    "#Aca aparece CTC Algorithm (Connectionist Temporal Classification) — Training and Inference,\n",
    "\n",
    "# CTC Loss (during Training):\n",
    "# It has a ground truth target transcript and tries to train the network to maximize the probability of outputting that correct transcript.\n",
    "\n",
    "# CTC Decoding (during Inference): \n",
    "# Here we don’t have a target transcript to refer to, and have to predict the most likely sequence of characters.\n",
    "# Use the character probabilities to pick the most likely character for each frame, including blanks. eg. “-G-o-ood”\n",
    "# 2 opciones:\n",
    "#     Greedy Search: Para cada periodo, simplemente elige el fonema de mayor probabilidad\n",
    "#    (Language Model) Beam Search: Para cada preiodo busca los fonemas que en relaciones a los que lo rodean, tienen mayor probabilidad de generar una palabra o frase. ej: HAC tiene mayor probabilidad que JAK en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dfa69b-141e-4c1f-839a-903626323267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medición y evaluación con Metrics — Word Error Rate (WER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8f80f-d81e-4567-a98a-dc7493bbab45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36784236-9930-4e33-a865-eec661886a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca28c9-450c-46fe-804b-bbed7b75aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
